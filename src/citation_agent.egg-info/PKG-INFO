Metadata-Version: 2.4
Name: citation-agent
Version: 0.1.0
Summary: CLI helper that uses OpenAI agents to extract and verify legal citations.
Author-email: Your Team <support@example.com>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: openai-agents>=0.5.0
Requires-Dist: typer>=0.15.1
Requires-Dist: pydantic>=2.9.0
Requires-Dist: rich>=13.9.4
Provides-Extra: pdf
Requires-Dist: pypdf>=5.1.0; extra == "pdf"
Provides-Extra: docx
Requires-Dist: python-docx>=1.1.2; extra == "docx"

# Citation Agent

Command-line helper that reviews a legal brief or memo with the [`openai-agents-python`](https://github.com/openai/openai-agents-python) SDK. It extracts every cited authority, checks whether the cited proposition is accurate, and flags hallucinated or weak citations before filing.

## Quick start

```bash
python3 -m venv .venv && source .venv/bin/activate
pip install -e .
export OPENAI_API_KEY=sk-...

# Run the audit (txt, md, pdf, or docx input)
citation-agent verify ./samples/brief.txt --model gpt-4.1-mini
```

Use `citation-agent verify --help` to see model, temperature, turn-limit, output-format, and web-search toggles. Add `--output json` when you need machine-readable results (the CLI otherwise renders a friendly Rich table).

## How it works

1. The file is normalized to text (with optional extras for `.pdf` and `.docx`).
2. The document is chunked and line-numbered so the agent can cite exact passages through custom tools:
   - `list_brief_sections` (paginate through chunks),
   - `get_brief_section` (return verbatim text with line numbers),
   - `search_brief_sections` (keyword lookup).
3. The agent (`CiteShield`) receives the entire numbered document plus access to the tools above. When enabled, it also gets the hosted `web_search` tool from OpenAI so it can confirm that each case or statute exists and supports the quoted rule.
4. The agent must return a strict [`CitationVerificationReport`](src/citation_agent/models.py) describing totals, risk, and a per-citation breakdown (status, reasoning, suggested fixes).

Because it is powered by OpenAI's Responses API underneath, the agent can call the base LLM multiple times, invoke the hosted web search tool, or reach any additional Model Context Protocol tools you wire up later (for example, connections to Westlaw, Lexis, or an internal know-how database).

## Capabilities & best-practice tools

* **LLM reasoning:** All auditing flows through the `gpt-4.1-mini` (configurable) model using the agents SDK, so you can mix fast and reasoning-tier models as needed.
* **Document navigation tools:** The bespoke section tools keep prompts short and encourage the model to cite exact line numbers.
* **Hosted web search:** Enable with `--web-search/--no-web-search`. This uses OpenAI's built-in search tool so the agent can confirm a case/statute exists even if it was hallucinated by the drafter.
* **Future MCP / vector tools:** The SDK natively supports Model Context Protocol servers, computer-use, code interpreter, and file-search/vector tools. Plugging one of those in is a natural next step if you want curated legal databases, docket review, or Shepard's-style checks.
* **Guardrails-ready:** The project already routes everything through the `CitationVerificationReport` schema; you can add guardrails or tripwires later (e.g., fail if more than N citations are unverified).

## Extending

* Hook up a private law-library MCP server by passing `mcp_servers=[...]` to the agent (see `agents.mcp`).
* Swap models per run (`--model gpt-4.1`, `--model o4-mini`, etc.).
* Build automation around the JSON output: block e-filing if `overall_assessment != "pass"`, log metrics, or create reminders.
* If you need better chunk recall, tune `chunk_document` in `src/citation_agent/document.py` or bolt on a proper vector store + `FileSearchTool`.

## Troubleshooting

| Problem | Fix |
| --- | --- |
| `OPENAI_API_KEY` missing | Export the key or use `.env` before running |
| PDF/Word import error | Install extras: `pip install citation-agent[pdf,docx]` |
| `MaxTurnsExceeded` | Raise `--max-turns` or simplify the brief (split large filings) |

Feel free to iterate on the instructions, add more custom tools, or wrap this CLI in your own GitHub workflow once you are satisfied with the results.
